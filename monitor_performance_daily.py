#!/usr/bin/env python
"""
–ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
–ó–∞–ø—É—Å–∫–∞—Ç—å —á–µ—Ä–µ–∑ cron: 0 9 * * * /path/to/python /path/to/monitor_performance_daily.py
"""

import os
import sys
import django
import json
from datetime import datetime
from pathlib import Path

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Django
sys.path.append(str(Path(__file__).parent))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'twocomms.settings')
django.setup()

from django.test import Client
from django.db import connection
from django.core.cache import cache
import requests

def measure_page_performance(url):
    """–ò–∑–º–µ—Ä—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    client = Client()
    
    # –°–±—Ä–æ—Å —Å—á–µ—Ç—á–∏–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
    initial_queries = len(connection.queries)
    
    # –ó–∞–ø—Ä–æ—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    start_time = datetime.now()
    response = client.get(url)
    end_time = datetime.now()
    
    # –ü–æ–¥—Å—á–µ—Ç –∑–∞–ø—Ä–æ—Å–æ–≤
    query_count = len(connection.queries) - initial_queries
    query_time = sum(float(q['time']) for q in connection.queries[-query_count:])
    
    response_time = (end_time - start_time).total_seconds() * 1000
    
    return {
        'url': url,
        'status_code': response.status_code,
        'response_time_ms': response_time,
        'query_count': query_count,
        'query_time_ms': query_time * 1000,
        'response_size_kb': len(response.content) / 1024,
    }

def check_core_web_vitals():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç Core Web Vitals —á–µ—Ä–µ–∑ PageSpeed Insights API"""
    # –¢—Ä–µ–±—É–µ—Ç—Å—è API –∫–ª—é—á –æ—Ç Google
    api_key = os.environ.get('PAGESPEED_API_KEY')
    if not api_key:
        return None
    
    url = 'https://twocomms.com.ua'
    api_url = f'https://www.googleapis.com/pagespeedonline/v5/runPagespeed?url={url}&key={api_key}'
    
    try:
        response = requests.get(api_url, timeout=30)
        data = response.json()
        
        lighthouse = data.get('lighthouseResult', {})
        metrics = lighthouse.get('audits', {}).get('metrics', {}).get('details', {}).get('items', [{}])[0]
        
        return {
            'fcp': metrics.get('firstContentfulPaint', 0) / 1000,
            'lcp': metrics.get('largestContentfulPaint', 0) / 1000,
            'tti': metrics.get('interactive', 0) / 1000,
            'cls': metrics.get('cumulativeLayoutShift', 0),
            'fid': metrics.get('maxPotentialFID', 0) / 1000,
            'score': lighthouse.get('categories', {}).get('performance', {}).get('score', 0) * 100,
        }
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ Core Web Vitals: {e}")
        return None

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    pages = [
        '/',
        '/catalog/',
        '/cart/',
    ]
    
    results = {
        'timestamp': datetime.now().isoformat(),
        'pages': [],
        'core_web_vitals': None,
    }
    
    # –ò–∑–º–µ—Ä–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å—Ç—Ä–∞–Ω–∏—Ü
    for page in pages:
        try:
            metrics = measure_page_performance(page)
            results['pages'].append(metrics)
            print(f"‚úÖ {page}: {metrics['response_time_ms']:.0f}ms, {metrics['query_count']} –∑–∞–ø—Ä–æ—Å–æ–≤")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–º–µ—Ä–µ–Ω–∏–∏ {page}: {e}")
            results['pages'].append({'url': page, 'error': str(e)})
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ Core Web Vitals
    cwv = check_core_web_vitals()
    if cwv:
        results['core_web_vitals'] = cwv
        print(f"‚úÖ Core Web Vitals: FCP={cwv['fcp']:.2f}s, LCP={cwv['lcp']:.2f}s, Score={cwv['score']:.0f}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    output_dir = Path('performance_monitoring')
    output_dir.mkdir(exist_ok=True)
    
    date_str = datetime.now().strftime('%Y%m%d')
    output_file = output_dir / f'performance_{date_str}.json'
    
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_file}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
    critical_issues = []
    
    for page in results['pages']:
        if 'error' in page:
            continue
        
        if page['response_time_ms'] > 2000:
            critical_issues.append(f"{page['url']}: –º–µ–¥–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç ({page['response_time_ms']:.0f}ms)")
        
        if page['query_count'] > 20:
            critical_issues.append(f"{page['url']}: —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤ ({page['query_count']})")
    
    if cwv:
        if cwv['fcp'] > 3.0:
            critical_issues.append(f"FCP –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –º–µ–¥–ª–µ–Ω–Ω—ã–π: {cwv['fcp']:.2f}s")
        if cwv['lcp'] > 4.0:
            critical_issues.append(f"LCP –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –º–µ–¥–ª–µ–Ω–Ω—ã–π: {cwv['lcp']:.2f}s")
        if cwv['cls'] > 0.25:
            critical_issues.append(f"CLS –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤—ã—Å–æ–∫–∏–π: {cwv['cls']:.2f}")
    
    if critical_issues:
        print("\n‚ö†Ô∏è  –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–†–û–ë–õ–ï–ú–´:")
        for issue in critical_issues:
            print(f"  - {issue}")
        
        # –û—Ç–ø—Ä–∞–≤–∏—Ç—å —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ (email, Telegram, Slack –∏ —Ç.–¥.)
        # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –æ—Ç–ø—Ä–∞–≤–∫—É —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π
    
    return 0 if not critical_issues else 1

if __name__ == '__main__':
    sys.exit(main())

