# robots.txt for TwoComms
# Документация: https://developers.google.com/search/docs/crawling-indexing/robots/intro

User-agent: *
Allow: /

# Disallow admin and приватные/служебные разделы
Disallow: /admin/
Disallow: /admin-panel/
Disallow: /debug/
Disallow: /media/debug/
Disallow: /cart/
Disallow: /checkout/
Disallow: /orders/
Disallow: /my/
Disallow: /login/
Disallow: /logout/
Disallow: /register/
Disallow: /profile/
Disallow: /api/
Disallow: /dev/
Disallow: /static/admin/
Disallow: /media/admin/

# Позволяем публичные страницы
Allow: /catalog/
Allow: /product/
Allow: /about/
Allow: /contacts/
Allow: /cooperation/
Allow: /delivery/
Allow: /favorites/
Allow: /static/
Allow: /media/

# Не индексировать страницы поиска и фильтров
Disallow: /search/

# Разрешаем пагинацию и ключевые параметры
Allow: /*?page=
Allow: /*?utm_=
Allow: /*?fbclid=

# Чистим только трекинговые параметры
Clean-param: utm_source&utm_medium&utm_campaign&utm_term&utm_content&utm_id&fbclid /

# Специальные правила для разных ботов
User-agent: Googlebot
Allow: /
Crawl-delay: 1

User-agent: Bingbot
Allow: /
Crawl-delay: 2

User-agent: YandexBot
Allow: /
Crawl-delay: 2

User-agent: facebookexternalhit
Allow: /

User-agent: Twitterbot
Allow: /

# Блокируем агрессивных ботов
User-agent: AhrefsBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: DotBot
Disallow: /

# Sitemap
Sitemap: https://twocomms.shop/sitemap.xml
Sitemap: https://www.twocomms.shop/sitemap.xml
Host: twocomms.shop
