#!/usr/bin/env python3
"""
–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π SEO –∞—É–¥–∏—Ç –¥–ª—è TwoComms (–±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π)
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã SEO –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–∞–π—Ç–∞
"""

import requests
import time
import json
import re
from urllib.parse import urljoin, urlparse
from datetime import datetime
import ssl
import socket
from typing import Dict, List, Tuple, Optional

class SimpleSEOAuditor:
    def __init__(self, base_url: str = "https://twocomms.shop"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.results = {
            'timestamp': datetime.now().isoformat(),
            'base_url': base_url,
            'technical_seo': {},
            'content_seo': {},
            'performance': {},
            'security': {},
            'issues': [],
            'recommendations': [],
            'score': 0
        }
    
    def check_http_status(self, url: str) -> Tuple[int, Dict]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç HTTP —Å—Ç–∞—Ç—É—Å –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏"""
        try:
            response = self.session.head(url, timeout=10, allow_redirects=True)
            return response.status_code, dict(response.headers)
        except Exception as e:
            return 0, {'error': str(e)}
    
    def get_page_content(self, url: str) -> Optional[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                return response.text
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è {url}: {e}")
        return None
    
    def extract_meta_tags(self, html: str) -> Dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞-—Ç–µ–≥–∏ –∏–∑ HTML"""
        meta_tags = {}
        
        # Title
        title_match = re.search(r'<title[^>]*>(.*?)</title>', html, re.IGNORECASE | re.DOTALL)
        if title_match:
            meta_tags['title'] = title_match.group(1).strip()
        
        # Description
        desc_match = re.search(r'<meta[^>]*name=["\']description["\'][^>]*content=["\']([^"\']*)["\']', html, re.IGNORECASE)
        if desc_match:
            meta_tags['description'] = desc_match.group(1)
        
        # Keywords
        keywords_match = re.search(r'<meta[^>]*name=["\']keywords["\'][^>]*content=["\']([^"\']*)["\']', html, re.IGNORECASE)
        if keywords_match:
            meta_tags['keywords'] = keywords_match.group(1)
        
        # Robots
        robots_match = re.search(r'<meta[^>]*name=["\']robots["\'][^>]*content=["\']([^"\']*)["\']', html, re.IGNORECASE)
        if robots_match:
            meta_tags['robots'] = robots_match.group(1)
        
        # Canonical
        canonical_match = re.search(r'<link[^>]*rel=["\']canonical["\'][^>]*href=["\']([^"\']*)["\']', html, re.IGNORECASE)
        if canonical_match:
            meta_tags['canonical'] = canonical_match.group(1)
        
        # Viewport
        viewport_match = re.search(r'<meta[^>]*name=["\']viewport["\'][^>]*content=["\']([^"\']*)["\']', html, re.IGNORECASE)
        if viewport_match:
            meta_tags['viewport'] = viewport_match.group(1)
        
        return meta_tags
    
    def extract_structured_data(self, html: str) -> Dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"""
        structured_data = {
            'json_ld_count': 0,
            'open_graph_count': 0,
            'twitter_cards_count': 0,
            'microdata_count': 0
        }
        
        # JSON-LD
        json_ld_matches = re.findall(r'<script[^>]*type=["\']application/ld\+json["\'][^>]*>(.*?)</script>', html, re.IGNORECASE | re.DOTALL)
        structured_data['json_ld_count'] = len(json_ld_matches)
        
        # Open Graph
        og_matches = re.findall(r'<meta[^>]*property=["\']og:[^"\']*["\']', html, re.IGNORECASE)
        structured_data['open_graph_count'] = len(og_matches)
        
        # Twitter Cards
        twitter_matches = re.findall(r'<meta[^>]*name=["\']twitter:[^"\']*["\']', html, re.IGNORECASE)
        structured_data['twitter_cards_count'] = len(twitter_matches)
        
        # Microdata
        microdata_matches = re.findall(r'itemscope', html, re.IGNORECASE)
        structured_data['microdata_count'] = len(microdata_matches)
        
        return structured_data
    
    def check_technical_seo(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã SEO"""
        print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ SEO...")
        
        tech_checks = {
            'robots_txt': self.check_robots_txt(),
            'sitemap': self.check_sitemap(),
            'ssl_certificate': self.check_ssl_certificate(),
            'redirects': self.check_redirects(),
            'page_speed': self.check_page_speed(),
            'mobile_friendly': self.check_mobile_friendly(),
            'structured_data': self.check_structured_data()
        }
        
        self.results['technical_seo'] = tech_checks
    
    def check_robots_txt(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç robots.txt"""
        url = f"{self.base_url}/robots.txt"
        status, headers = self.check_http_status(url)
        
        if status == 200:
            try:
                response = self.session.get(url)
                content = response.text
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                has_sitemap = 'sitemap' in content.lower()
                has_disallow = 'disallow' in content.lower()
                has_allow = 'allow' in content.lower()
                
                return {
                    'status': 'ok',
                    'accessible': True,
                    'has_sitemap': has_sitemap,
                    'has_disallow': has_disallow,
                    'has_allow': has_allow,
                    'content_length': len(content),
                    'headers': headers
                }
            except Exception as e:
                return {'status': 'error', 'error': str(e)}
        else:
            return {'status': 'error', 'http_status': status}
    
    def check_sitemap(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç sitemap.xml"""
        url = f"{self.base_url}/sitemap.xml"
        status, headers = self.check_http_status(url)
        
        if status == 200:
            try:
                response = self.session.get(url)
                content = response.text
                
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º URL
                url_matches = re.findall(r'<url>', content)
                url_count = len(url_matches)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–º–µ–Ω –≤ URL
                correct_domain = self.base_url.replace('https://', '') in content
                
                return {
                    'status': 'ok',
                    'accessible': True,
                    'url_count': url_count,
                    'correct_domain': correct_domain,
                    'content_type': headers.get('content-type', ''),
                    'content_length': len(content)
                }
            except Exception as e:
                return {'status': 'error', 'error': str(e)}
        else:
            return {'status': 'error', 'http_status': status}
    
    def check_ssl_certificate(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç SSL —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç"""
        try:
            hostname = urlparse(self.base_url).hostname
            context = ssl.create_default_context()
            
            with socket.create_connection((hostname, 443), timeout=10) as sock:
                with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                    cert = ssock.getpeercert()
                    
                    return {
                        'status': 'ok',
                        'has_ssl': True,
                        'subject': dict(x[0] for x in cert['subject']),
                        'issuer': dict(x[0] for x in cert['issuer']),
                        'version': cert['version'],
                        'serial_number': cert['serialNumber']
                    }
        except Exception as e:
            return {'status': 'error', 'error': str(e)}
    
    def check_redirects(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º HTTP -> HTTPS —Ä–µ–¥–∏—Ä–µ–∫—Ç
        http_url = self.base_url.replace('https://', 'http://')
        status, headers = self.check_http_status(http_url)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º www —Ä–µ–¥–∏—Ä–µ–∫—Ç
        www_url = self.base_url.replace('://', '://www.')
        www_status, www_headers = self.check_http_status(www_url)
        
        return {
            'http_to_https': {
                'status': status,
                'redirects_to_https': status in [301, 302, 307, 308]
            },
            'www_redirect': {
                'status': www_status,
                'accessible': www_status == 200
            }
        }
    
    def check_page_speed(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü"""
        pages = ['/', '/catalog/', '/about/']
        results = {}
        
        for page in pages:
            url = f"{self.base_url}{page}"
            start_time = time.time()
            
            try:
                response = self.session.get(url, timeout=10)
                load_time = time.time() - start_time
                
                results[page] = {
                    'load_time': round(load_time, 2),
                    'status_code': response.status_code,
                    'content_length': len(response.content),
                    'fast_load': load_time < 3.0
                }
            except Exception as e:
                results[page] = {'error': str(e)}
        
        return results
    
    def check_mobile_friendly(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –º–æ–±–∏–ª—å–Ω—É—é –∞–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å"""
        url = f"{self.base_url}/"
        html = self.get_page_content(url)
        
        if html:
            meta_tags = self.extract_meta_tags(html)
            has_viewport = 'viewport' in meta_tags
            
            return {
                'has_viewport_meta': has_viewport,
                'viewport_content': meta_tags.get('viewport', ''),
                'responsive_design': has_viewport
            }
        
        return {'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É'}
    
    def check_structured_data(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"""
        url = f"{self.base_url}/"
        html = self.get_page_content(url)
        
        if html:
            structured_data = self.extract_structured_data(html)
            
            return {
                **structured_data,
                'has_structured_data': structured_data['json_ld_count'] > 0 or structured_data['microdata_count'] > 0,
                'has_social_meta': structured_data['open_graph_count'] > 0 or structured_data['twitter_cards_count'] > 0
            }
        
        return {'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É'}
    
    def check_content_seo(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã SEO"""
        print("üìù –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ SEO...")
        
        content_checks = {
            'meta_tags': self.check_meta_tags(),
            'headings': self.check_headings(),
            'images': self.check_images(),
            'content_quality': self.check_content_quality()
        }
        
        self.results['content_seo'] = content_checks
    
    def check_meta_tags(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –º–µ—Ç–∞-—Ç–µ–≥–∏"""
        url = f"{self.base_url}/"
        html = self.get_page_content(url)
        
        if html:
            meta_tags = self.extract_meta_tags(html)
            
            return {
                'has_title': 'title' in meta_tags,
                'title_text': meta_tags.get('title', ''),
                'title_length': len(meta_tags.get('title', '')),
                'title_optimal': 30 <= len(meta_tags.get('title', '')) <= 60,
                'has_description': 'description' in meta_tags,
                'description_text': meta_tags.get('description', ''),
                'description_length': len(meta_tags.get('description', '')),
                'description_optimal': 120 <= len(meta_tags.get('description', '')) <= 160,
                'has_keywords': 'keywords' in meta_tags,
                'keywords_text': meta_tags.get('keywords', ''),
                'has_canonical': 'canonical' in meta_tags,
                'canonical_url': meta_tags.get('canonical', ''),
                'has_robots_meta': 'robots' in meta_tags,
                'robots_content': meta_tags.get('robots', ''),
                'allows_indexing': 'noindex' not in meta_tags.get('robots', '').lower()
            }
        
        return {'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É'}
    
    def check_headings(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤"""
        url = f"{self.base_url}/"
        html = self.get_page_content(url)
        
        if html:
            h1_matches = re.findall(r'<h1[^>]*>(.*?)</h1>', html, re.IGNORECASE | re.DOTALL)
            h2_matches = re.findall(r'<h2[^>]*>(.*?)</h2>', html, re.IGNORECASE | re.DOTALL)
            h3_matches = re.findall(r'<h3[^>]*>(.*?)</h3>', html, re.IGNORECASE | re.DOTALL)
            
            return {
                'h1_count': len(h1_matches),
                'h1_texts': [h.strip() for h in h1_matches],
                'h2_count': len(h2_matches),
                'h3_count': len(h3_matches),
                'has_proper_h1': len(h1_matches) == 1,
                'heading_structure_ok': len(h1_matches) == 1 and len(h1_matches) > 0
            }
        
        return {'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É'}
    
    def check_images(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        url = f"{self.base_url}/"
        html = self.get_page_content(url)
        
        if html:
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ img —Ç–µ–≥–∏
            img_matches = re.findall(r'<img[^>]*>', html, re.IGNORECASE)
            total_images = len(img_matches)
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å alt
            images_with_alt = 0
            for img in img_matches:
                if 'alt=' in img.lower():
                    images_with_alt += 1
            
            images_without_alt = total_images - images_with_alt
            
            return {
                'total_images': total_images,
                'images_with_alt': images_with_alt,
                'images_without_alt': images_without_alt,
                'alt_coverage': round((images_with_alt / total_images * 100), 2) if total_images > 0 else 0,
                'all_images_have_alt': images_without_alt == 0
            }
        
        return {'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É'}
    
    def check_content_quality(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        url = f"{self.base_url}/"
        html = self.get_page_content(url)
        
        if html:
            # –£–±–∏—Ä–∞–µ–º HTML —Ç–µ–≥–∏ –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Å–ª–æ–≤
            text = re.sub(r'<[^>]+>', ' ', html)
            text = re.sub(r'\s+', ' ', text).strip()
            word_count = len(text.split())
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
            paragraph_matches = re.findall(r'<p[^>]*>', html, re.IGNORECASE)
            paragraph_count = len(paragraph_matches)
            
            return {
                'word_count': word_count,
                'paragraph_count': paragraph_count,
                'has_substantial_content': word_count > 300,
                'content_density_ok': word_count > 100
            }
        
        return {'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É'}
    
    def calculate_seo_score(self):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ–±—â–∏–π SEO –±–∞–ª–ª"""
        score = 0
        max_score = 100
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã (40 –±–∞–ª–ª–æ–≤)
        tech_score = 0
        tech_max = 40
        
        if self.results['technical_seo'].get('robots_txt', {}).get('accessible'):
            tech_score += 5
        if self.results['technical_seo'].get('sitemap', {}).get('accessible'):
            tech_score += 5
        if self.results['technical_seo'].get('ssl_certificate', {}).get('has_ssl'):
            tech_score += 5
        if self.results['technical_seo'].get('mobile_friendly', {}).get('responsive_design'):
            tech_score += 5
        if self.results['technical_seo'].get('structured_data', {}).get('has_structured_data'):
            tech_score += 5
        if self.results['content_seo'].get('meta_tags', {}).get('has_canonical'):
            tech_score += 5
        if self.results['content_seo'].get('meta_tags', {}).get('allows_indexing'):
            tech_score += 5
        if self.results['technical_seo'].get('page_speed', {}).get('/', {}).get('fast_load'):
            tech_score += 5
        
        # –ö–æ–Ω—Ç–µ–Ω—Ç–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã (35 –±–∞–ª–ª–æ–≤)
        content_score = 0
        content_max = 35
        
        meta_tags = self.results['content_seo'].get('meta_tags', {})
        if meta_tags.get('has_title') and meta_tags.get('title_optimal'):
            content_score += 8
        if meta_tags.get('has_description') and meta_tags.get('description_optimal'):
            content_score += 8
        if self.results['content_seo'].get('headings', {}).get('has_proper_h1'):
            content_score += 5
        if self.results['content_seo'].get('images', {}).get('all_images_have_alt'):
            content_score += 7
        if self.results['content_seo'].get('content_quality', {}).get('has_substantial_content'):
            content_score += 7
        
        # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (25 –±–∞–ª–ª–æ–≤)
        perf_score = 0
        perf_max = 25
        
        page_speed = self.results['technical_seo'].get('page_speed', {})
        for page in ['/', '/catalog/', '/about/']:
            if page_speed.get(page, {}).get('fast_load'):
                perf_score += 8
        
        total_score = tech_score + content_score + perf_score
        self.results['score'] = round((total_score / (tech_max + content_max + perf_max)) * 100, 1)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å
        if self.results['score'] >= 90:
            level = "–û—Ç–ª–∏—á–Ω–æ"
        elif self.results['score'] >= 80:
            level = "–•–æ—Ä–æ—à–æ"
        elif self.results['score'] >= 70:
            level = "–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ"
        elif self.results['score'] >= 60:
            level = "–¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è"
        else:
            level = "–ö—Ä–∏—Ç–∏—á–Ω–æ"
        
        self.results['level'] = level
    
    def generate_recommendations(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é"""
        recommendations = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º robots.txt
        if not self.results['technical_seo'].get('robots_txt', {}).get('accessible'):
            recommendations.append("üîß –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª robots.txt –≤ –∫–æ—Ä–Ω–µ —Å–∞–π—Ç–∞")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º sitemap
        sitemap = self.results['technical_seo'].get('sitemap', {})
        if not sitemap.get('accessible'):
            recommendations.append("üó∫Ô∏è –°–æ–∑–¥–∞–π—Ç–µ sitemap.xml –¥–ª—è –ª—É—á—à–µ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
        elif not sitemap.get('correct_domain'):
            recommendations.append("üåê –ò—Å–ø—Ä–∞–≤—å—Ç–µ –¥–æ–º–µ–Ω –≤ sitemap.xml (—Å–µ–π—á–∞—Å example.com)")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ—Ç–∞-—Ç–µ–≥–∏
        meta_tags = self.results['content_seo'].get('meta_tags', {})
        if not meta_tags.get('has_title'):
            recommendations.append("üìù –î–æ–±–∞–≤—å—Ç–µ —Ç–µ–≥ <title> –Ω–∞ –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
        elif not meta_tags.get('title_optimal'):
            recommendations.append("üìè –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ –¥–ª–∏–Ω—É title (30-60 —Å–∏–º–≤–æ–ª–æ–≤)")
        
        if not meta_tags.get('has_description'):
            recommendations.append("üìÑ –î–æ–±–∞–≤—å—Ç–µ –º–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
        elif not meta_tags.get('description_optimal'):
            recommendations.append("üìè –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ –¥–ª–∏–Ω—É description (120-160 —Å–∏–º–≤–æ–ª–æ–≤)")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        images = self.results['content_seo'].get('images', {})
        if not images.get('all_images_have_alt'):
            recommendations.append("üñºÔ∏è –î–æ–±–∞–≤—å—Ç–µ alt-–∞—Ç—Ä–∏–±—É—Ç—ã –∫–æ –≤—Å–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
        headings = self.results['content_seo'].get('headings', {})
        if not headings.get('has_proper_h1'):
            recommendations.append("üìã –î–æ–±–∞–≤—å—Ç–µ –æ–¥–∏–Ω —Ç–µ–≥ H1 –Ω–∞ –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–±–∏–ª—å–Ω—É—é –∞–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å
        if not self.results['technical_seo'].get('mobile_friendly', {}).get('responsive_design'):
            recommendations.append("üì± –î–æ–±–∞–≤—å—Ç–µ viewport meta-—Ç–µ–≥ –¥–ª—è –º–æ–±–∏–ª—å–Ω–æ–π –∞–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç–∏")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å
        page_speed = self.results['technical_seo'].get('page_speed', {})
        slow_pages = [page for page, data in page_speed.items() if not data.get('fast_load', True)]
        if slow_pages:
            recommendations.append(f"‚ö° –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ —Å–∫–æ—Ä–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü: {', '.join(slow_pages)}")
        
        self.results['recommendations'] = recommendations
    
    def run_full_audit(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π SEO –∞—É–¥–∏—Ç"""
        print("üöÄ –ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ SEO –∞—É–¥–∏—Ç–∞ –¥–ª—è TwoComms...")
        print("=" * 60)
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        self.check_technical_seo()
        
        # –ö–æ–Ω—Ç–µ–Ω—Ç–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        self.check_content_seo()
        
        # –í—ã—á–∏—Å–ª—è–µ–º –±–∞–ª–ª
        self.calculate_seo_score()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        self.generate_recommendations()
        
        print("‚úÖ –ê—É–¥–∏—Ç –∑–∞–≤–µ—Ä—à–µ–Ω!")
        return self.results
    
    def save_report(self, filename: str = None):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—á–µ—Ç –≤ —Ñ–∞–π–ª"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"seo_audit_report_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"üìä –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {filename}")
        return filename
    
    def print_summary(self):
        """–í—ã–≤–æ–¥–∏—Ç –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É"""
        print("\n" + "=" * 60)
        print("üìä –°–í–û–î–ö–ê SEO –ê–£–î–ò–¢–ê")
        print("=" * 60)
        print(f"üåê –°–∞–π—Ç: {self.results['base_url']}")
        print(f"üìÖ –î–∞—Ç–∞: {self.results['timestamp']}")
        print(f"üéØ –û–±—â–∏–π –±–∞–ª–ª: {self.results['score']}/100 ({self.results['level']})")
        
        print(f"\nüîß –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã:")
        tech = self.results['technical_seo']
        print(f"  ‚Ä¢ Robots.txt: {'‚úÖ' if tech.get('robots_txt', {}).get('accessible') else '‚ùå'}")
        print(f"  ‚Ä¢ Sitemap: {'‚úÖ' if tech.get('sitemap', {}).get('accessible') else '‚ùå'}")
        print(f"  ‚Ä¢ SSL: {'‚úÖ' if tech.get('ssl_certificate', {}).get('has_ssl') else '‚ùå'}")
        print(f"  ‚Ä¢ –ú–æ–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è: {'‚úÖ' if tech.get('mobile_friendly', {}).get('responsive_design') else '‚ùå'}")
        print(f"  ‚Ä¢ –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {'‚úÖ' if tech.get('structured_data', {}).get('has_structured_data') else '‚ùå'}")
        
        print(f"\nüìù –ö–æ–Ω—Ç–µ–Ω—Ç–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã:")
        content = self.results['content_seo']
        print(f"  ‚Ä¢ Title: {'‚úÖ' if content.get('meta_tags', {}).get('has_title') else '‚ùå'}")
        print(f"  ‚Ä¢ Description: {'‚úÖ' if content.get('meta_tags', {}).get('has_description') else '‚ùå'}")
        print(f"  ‚Ä¢ H1 –∑–∞–≥–æ–ª–æ–≤–æ–∫: {'‚úÖ' if content.get('headings', {}).get('has_proper_h1') else '‚ùå'}")
        print(f"  ‚Ä¢ Alt-–∞—Ç—Ä–∏–±—É—Ç—ã: {'‚úÖ' if content.get('images', {}).get('all_images_have_alt') else '‚ùå'}")
        
        if self.results['recommendations']:
            print(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ ({len(self.results['recommendations'])}):")
            for i, rec in enumerate(self.results['recommendations'], 1):
                print(f"  {i}. {rec}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üîç –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π SEO –∞—É–¥–∏—Ç TwoComms")
    print("=" * 60)
    
    auditor = SimpleSEOAuditor()
    results = auditor.run_full_audit()
    
    # –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É
    auditor.print_summary()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
    report_file = auditor.save_report()
    
    print(f"\nüéâ –ê—É–¥–∏—Ç –∑–∞–≤–µ—Ä—à–µ–Ω! –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {report_file}")
    
    return results


if __name__ == "__main__":
    main()
